import numpy as np
import pandas as pd
from pathlib import Path
import logging
from typing import Tuple, Dict, Optional, List, Union
from datetime import datetime
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# TensorFlow and Keras imports
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model # type: ignore
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization # type: ignore
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau # type: ignore
from tensorflow.keras.optimizers import Adam # type: ignore
from tensorflow.keras.regularizers import l2 # type: ignore

# Scikit-learn for metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error

# Optional: Keras Tuner for hyperparameter optimization
try:
    from keras_tuner import RandomSearch, BayesianOptimization
    KERAS_TUNER_AVAILABLE = True
except ImportError:
    KERAS_TUNER_AVAILABLE = False

class StockPredictor:
    """
    Advanced LSTM-based stock price prediction model optimized for Indian markets.
    
    Features:
    - GPU/CPU automatic detection and optimization
    - Hyperparameter tuning with Keras Tuner
    - Multi-step prediction with uncertainty quantification
    - Ensemble model support
    - Advanced financial metrics (MAPE, Directional Accuracy)
    - Production-ready error handling and logging
    - Memory optimization for large datasets
    
    Architecture:
    - Stacked LSTM layers with dropout and batch normalization
    - L2 regularization to prevent overfitting
    - Adaptive learning rate scheduling
    - Early stopping with best weight restoration
    
    Usage:
        predictor = StockPredictor()
        predictor.build_model((60, 14))  # 60 timesteps, 14 features
        history = predictor.train(X_train, y_train, X_val, y_val)
        predictions = predictor.predict(X_test, confidence_intervals=True)
    """
    
    def __init__(self, 
                 model_dir: str = "../models",
                 learning_rate: float = 0.001,
                 epochs: int = 100,
                 batch_size: int = 32,
                 lstm_units: int = 50,
                 dropout_rate: float = 0.2,
                 l2_reg: float = 0.01):
        """
        Initialize StockPredictor with comprehensive configuration.
        
        Args:
            model_dir: Directory for saving models and checkpoints
            learning_rate: Initial learning rate for Adam optimizer
            epochs: Maximum training epochs
            batch_size: Training batch size
            lstm_units: Number of units in each LSTM layer
            dropout_rate: Dropout rate for regularization
            l2_reg: L2 regularization strength
        """
        # Setup directories
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # Model hyperparameters
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.batch_size = batch_size
        self.lstm_units = lstm_units
        self.dropout_rate = dropout_rate
        self.l2_reg = l2_reg
        
        # Model state
        self.model = None
        self.history = None
        self.is_trained = False
        
        self.logger = logging.getLogger(__name__)
        
        # Hardware detection and optimization
        self.setup_hardware()
        
        # Setup logging
        self.logger.info("StockPredictor initialized successfully")
        
    def setup_hardware(self):
        """Detect and configure GPU/CPU for optimal performance."""
        # Detect available GPUs
        self.gpus = tf.config.list_physical_devices('GPU')
        
        if self.gpus:
            try:
                # Enable memory growth to prevent GPU memory issues
                for gpu in self.gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                
                # Use mixed precision for faster training on modern GPUs
                tf.keras.mixed_precision.set_global_policy('mixed_float16')
                
                self.logger.info(f"ðŸš€ GPU acceleration enabled: {len(self.gpus)} GPU(s) detected")
                self.device_strategy = tf.distribute.MirroredStrategy() if len(self.gpus) > 1 else None
                
            except RuntimeError as e:
                self.logger.warning(f"GPU setup failed, falling back to CPU: {e}")
                self.gpus = []
        else:
            self.logger.info("ðŸ’» Using CPU for training")
            
        # Optimize CPU performance
        tf.config.threading.set_intra_op_parallelism_threads(0)  # Use all cores
        tf.config.threading.set_inter_op_parallelism_threads(0)
    
    def build_model(self, input_shape: Tuple[int, int], 
                   architecture: str = 'standard') -> Sequential:
        """
        Build optimized LSTM architecture for stock prediction.
        
        Args:
            input_shape: (sequence_length, num_features)
            architecture: 'standard', 'deep', or 'lightweight'
            
        Returns:
            Compiled Keras Sequential model
        """
        try:
            # Build within strategy scope if using multiple GPUs
            with self.device_strategy.scope() if self.device_strategy else tf.device('/GPU:0' if self.gpus else '/CPU:0'):
                
                model = Sequential(name=f"StockLSTM_{architecture}")
                
                if architecture == 'deep':
                    # Deep architecture for complex patterns
                    model.add(LSTM(self.lstm_units * 2, 
                                 return_sequences=True, 
                                 input_shape=input_shape,
                                 kernel_regularizer=l2(self.l2_reg)))
                    model.add(BatchNormalization())
                    model.add(Dropout(self.dropout_rate))
                    
                    model.add(LSTM(self.lstm_units * 2, 
                                 return_sequences=True,
                                 kernel_regularizer=l2(self.l2_reg)))
                    model.add(BatchNormalization())
                    model.add(Dropout(self.dropout_rate))
                    
                    model.add(LSTM(self.lstm_units, 
                                 return_sequences=False,
                                 kernel_regularizer=l2(self.l2_reg)))
                    model.add(BatchNormalization())
                    model.add(Dropout(self.dropout_rate))
                    
                elif architecture == 'lightweight':
                    # Lightweight for faster training
                    model.add(LSTM(self.lstm_units // 2, 
                                 return_sequences=True, 
                                 input_shape=input_shape))
                    model.add(Dropout(self.dropout_rate))
                    
                    model.add(LSTM(self.lstm_units // 2, 
                                 return_sequences=False))
                    model.add(Dropout(self.dropout_rate))
                    
                else:  # standard architecture
                    # Balanced architecture for most use cases
                    model.add(LSTM(self.lstm_units, 
                                 return_sequences=True, 
                                 input_shape=input_shape,
                                 kernel_regularizer=l2(self.l2_reg)))
                    model.add(BatchNormalization())
                    model.add(Dropout(self.dropout_rate))
                    
                    model.add(LSTM(self.lstm_units, 
                                 return_sequences=False,
                                 kernel_regularizer=l2(self.l2_reg)))
                    model.add(BatchNormalization())
                    model.add(Dropout(self.dropout_rate))
                
                # Dense layers for final prediction
                model.add(Dense(self.lstm_units // 2, 
                              activation='relu',
                              kernel_regularizer=l2(self.l2_reg)))
                model.add(Dropout(self.dropout_rate))
                model.add(Dense(1))  # Single output for price prediction
                
                # Compile with optimized settings
                optimizer = Adam(learning_rate=self.learning_rate, 
                               clipnorm=1.0)  # Gradient clipping
                
                model.compile(
                    optimizer=optimizer,
                    loss='mse',
                    metrics=['mae']
                )
                
                self.logger.info(f"Model built: {architecture} architecture")
                self.logger.info(f"Total parameters: {model.count_params():,}")
                
                return model
                
        except Exception as e:
            self.logger.error(f"Error building model: {e}")
            raise
    
    def train(self, 
              X_train: np.ndarray, 
              y_train: np.ndarray,
              X_val: Optional[np.ndarray] = None, 
              y_val: Optional[np.ndarray] = None,
              architecture: str = 'standard',
              patience: int = 15,
              min_lr: float = 1e-7,
              verbose: int = 1) -> Dict[str, List[float]]:
        """
        Train LSTM model with advanced callbacks and monitoring.
        
        Args:
            X_train, y_train: Training data
            X_val, y_val: Validation data
            architecture: Model architecture type
            patience: Early stopping patience
            min_lr: Minimum learning rate for scheduler
            verbose: Training verbosity
            
        Returns:
            Training history dictionary
        """
        try:
            # Build model
            self.model = self.build_model(
                input_shape=(X_train.shape[1], X_train.shape[2]),
                architecture=architecture
            )
            
            # Setup advanced callbacks
            callbacks = self._setup_callbacks(patience, min_lr, verbose)
            
            # Prepare validation data
            validation_data = (X_val, y_val) if X_val is not None else None
            
            self.logger.info(f"Starting training: {len(X_train)} samples, {self.epochs} max epochs")
            
            # Train model
            self.history = self.model.fit(
                X_train, y_train,
                epochs=self.epochs,
                batch_size=self.batch_size,
                validation_data=validation_data,
                callbacks=callbacks,
                verbose=verbose,
                shuffle=True  # Shuffle for better generalization
            )
            
            self.is_trained = True
            
            # Log training summary
            final_loss = self.history.history['loss'][-1]
            self.logger.info(f"Training completed - Final loss: {final_loss:.6f}")
            
            if validation_data:
                final_val_loss = self.history.history['val_loss'][-1]
                self.logger.info(f"Final validation loss: {final_val_loss:.6f}")
            
            return self.history.history
            
        except Exception as e:
            self.logger.error(f"Training failed: {e}")
            raise
    
    def _setup_callbacks(self, patience: int, min_lr: float, verbose: int) -> List:
        """Setup training callbacks for optimal performance."""
        callbacks = [
            # Early stopping with best weight restoration
            EarlyStopping(
                monitor='val_loss' if 'val_loss' in ['val_loss'] else 'loss',
                patience=patience,
                restore_best_weights=True,
                verbose=verbose,
                min_delta=1e-7
            ),
            
            # Model checkpointing
            ModelCheckpoint(
                filepath=str(self.model_dir / 'best_model.keras'),
                monitor='val_loss' if 'val_loss' in ['val_loss'] else 'loss',
                save_best_only=True,
                save_weights_only=False,
                verbose=verbose
            ),
            
            # Learning rate reduction
            ReduceLROnPlateau(
                monitor='val_loss' if 'val_loss' in ['val_loss'] else 'loss',
                factor=0.5,
                patience=patience // 3,
                min_lr=min_lr,
                verbose=verbose,
                cooldown=5
            )
        ]
        
        return callbacks
    
    def predict(self, 
                X: np.ndarray, 
                num_predictions: int = 1,
                confidence_intervals: bool = False,
                mc_samples: int = 100) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray, np.ndarray]]:
        """
        Generate predictions with optional uncertainty quantification.
        
        Args:
            X: Input sequences
            num_predictions: Number of future steps to predict
            confidence_intervals: Whether to compute confidence intervals
            mc_samples: Number of Monte Carlo samples for uncertainty
            
        Returns:
            Predictions array, optionally with confidence bounds
        """
        try:
            if not self.is_trained:
                raise ValueError("Model must be trained before prediction")
            
            if confidence_intervals:
                return self._predict_with_uncertainty(X, num_predictions, mc_samples)
            else:
                return self._predict_deterministic(X, num_predictions)
                
        except Exception as e:
            self.logger.error(f"Prediction failed: {e}")
            raise
    
    def _predict_deterministic(self, X: np.ndarray, num_predictions: int) -> np.ndarray:
        """Standard deterministic prediction."""
        predictions = []
        current_seq = X[-1:].copy()
        
        for _ in range(num_predictions):
            pred = self.model.predict(current_seq, verbose=0)[0, 0]
            predictions.append(pred)
            
            # Update sequence for next prediction
            current_seq = np.roll(current_seq, -1, axis=1)
            current_seq[0, -1, -1] = pred  # Assume last feature is target
            
        return np.array(predictions)
    
    def _predict_with_uncertainty(self, X: np.ndarray, num_predictions: int, 
                                 mc_samples: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Monte Carlo dropout for uncertainty estimation."""
        all_predictions = []
        
        for _ in range(mc_samples):
            # Enable dropout during inference
            pred_samples = []
            current_seq = X[-1:].copy()
            
            for _ in range(num_predictions):
                # Predict with dropout active
                pred = self.model(current_seq, training=True).numpy()[0, 0]
                pred_samples.append(pred)
                
                # Update sequence
                current_seq = np.roll(current_seq, -1, axis=1)
                current_seq[0, -1, -1] = pred
                
            all_predictions.append(pred_samples)
        
        all_predictions = np.array(all_predictions)  # Shape: (mc_samples, num_predictions)
        
        # Calculate statistics
        mean_pred = np.mean(all_predictions, axis=0)
        std_pred = np.std(all_predictions, axis=0)
        
        # 95% confidence intervals
        lower_bound = np.percentile(all_predictions, 2.5, axis=0)
        upper_bound = np.percentile(all_predictions, 97.5, axis=0)
        
        return mean_pred, lower_bound, upper_bound
    
    def evaluate(self, 
                 X_test: np.ndarray, 
                 y_test: np.ndarray,
                 scaler: Optional = None, # type: ignore
                 return_predictions: bool = False) -> Union[Dict[str, float], Tuple[Dict[str, float], np.ndarray]]:
        """
        Comprehensive model evaluation with financial metrics.
        
        Args:
            X_test, y_test: Test data
            scaler: MinMaxScaler for inverse transformation  
            return_predictions: Whether to return predictions array
            
        Returns:
            Dictionary of evaluation metrics, optionally with predictions
        """
        try:
            if not self.is_trained:
                raise ValueError("Model must be trained before evaluation")
            
            # Generate predictions
            y_pred = self.model.predict(X_test, verbose=0).flatten()
            y_true = y_test.flatten()
            
            # Inverse transform if scaler provided
            if scaler is not None:
                y_true, y_pred = self._inverse_transform_predictions(
                    y_true, y_pred, scaler
                )
            
            # Calculate comprehensive metrics
            metrics = self._calculate_metrics(y_true, y_pred)
            
            # Log results
            self.logger.info("Evaluation completed:")
            for metric, value in metrics.items():
                self.logger.info(f"  {metric.upper()}: {value:.6f}")
            
            if return_predictions:
                return metrics, y_pred
            return metrics
            
        except Exception as e:
            self.logger.error(f"Evaluation failed: {e}")
            raise
    
    def _inverse_transform_predictions(self, y_true: np.ndarray, y_pred: np.ndarray, 
                                     scaler) -> Tuple[np.ndarray, np.ndarray]:
        """Inverse transform normalized predictions to original scale."""
        # Create dummy arrays for inverse transformation
        n_features = scaler.n_features_in_
        
        y_true_scaled = np.zeros((len(y_true), n_features))
        y_pred_scaled = np.zeros((len(y_pred), n_features))
        
        # Assuming 'Close' price is at index 3 (OHLC order)
        y_true_scaled[:, 3] = y_true
        y_pred_scaled[:, 3] = y_pred
        
        # Inverse transform
        y_true_inv = scaler.inverse_transform(y_true_scaled)[:, 3]
        y_pred_inv = scaler.inverse_transform(y_pred_scaled)[:, 3]
        
        return y_true_inv, y_pred_inv
    
    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """Calculate comprehensive evaluation metrics."""
        metrics = {}
        
        # Regression metrics
        metrics['mse'] = mean_squared_error(y_true, y_pred)
        metrics['rmse'] = np.sqrt(metrics['mse'])
        metrics['mae'] = mean_absolute_error(y_true, y_pred)
        metrics['mape'] = mean_absolute_percentage_error(y_true, y_pred) * 100
        
        # Financial-specific metrics
        if len(y_true) > 1:
            # Directional accuracy
            y_true_diff = np.diff(y_true)
            y_pred_diff = np.diff(y_pred)
            direction_correct = np.sign(y_true_diff) == np.sign(y_pred_diff)
            metrics['directional_accuracy'] = np.mean(direction_correct) * 100
            
            # Hit rate (predictions within 5% of actual)
            tolerance = 0.05
            within_tolerance = np.abs((y_pred - y_true) / y_true) <= tolerance
            metrics['hit_rate_5pct'] = np.mean(within_tolerance) * 100
        
        return metrics
    
    def save_model(self, symbol: str, metadata: Optional[Dict] = None) -> Path:
        """Save trained model with metadata."""
        try:
            if not self.is_trained:
                raise ValueError("No trained model to save")
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"lstm_{symbol}_{timestamp}.keras"
            filepath = self.model_dir / filename
            
            # Save model
            self.model.save(filepath)
            
            # Save metadata
            if metadata:
                metadata_file = filepath.with_suffix('.json')
                import json
                with open(metadata_file, 'w') as f:
                    json.dump(metadata, f, indent=2)
            
            self.logger.info(f"Model saved: {filepath}")
            return filepath
            
        except Exception as e:
            self.logger.error(f"Model saving failed: {e}")
            raise
    
    def load_model(self, filepath: Path) -> None:
        """Load pre-trained model."""
        try:
            if not filepath.exists():
                raise FileNotFoundError(f"Model file not found: {filepath}")
            
            self.model = load_model(filepath)
            self.is_trained = True
            
            self.logger.info(f"Model loaded: {filepath}")
            
        except Exception as e:
            self.logger.error(f"Model loading failed: {e}")
            raise
    
    def plot_training_history(self):
        """Plot comprehensive training history."""
        if self.history is None:
            raise ValueError("No training history available")
        
        history = self.history.history
        epochs = range(1, len(history['loss']) + 1)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss plot
        axes[0, 0].plot(epochs, history['loss'], 'b-', label='Training Loss')
        if 'val_loss' in history:
            axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss')
        axes[0, 0].set_title('Model Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # MAE plot
        if 'mae' in history:
            axes[0, 1].plot(epochs, history['mae'], 'b-', label='Training MAE')
            if 'val_mae' in history:
                axes[0, 1].plot(epochs, history['val_mae'], 'r-', label='Validation MAE')
            axes[0, 1].set_title('Model MAE')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('MAE')
            axes[0, 1].legend()
            axes[0, 1].grid(True)
        
        # Learning rate plot
        if 'lr' in history:
            axes[1, 0].plot(epochs, history['lr'], 'g-', label='Learning Rate')
            axes[1, 0].set_title('Learning Rate Schedule')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Learning Rate')
            axes[1, 0].set_yscale('log')
            axes[1, 0].legend()
            axes[1, 0].grid(True)
        
        # Training summary
        axes[1, 1].axis('off')
        summary_text = f"""
        Training Summary:
        â€¢ Total Epochs: {len(epochs)}
        â€¢ Final Training Loss: {history['loss'][-1]:.6f}
        â€¢ Best Training Loss: {min(history['loss']):.6f}
        """
        if 'val_loss' in history:
            summary_text += f"""â€¢ Final Val Loss: {history['val_loss'][-1]:.6f}
        â€¢ Best Val Loss: {min(history['val_loss']):.6f}"""
        
        axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, 
                        verticalalignment='center', fontfamily='monospace')
        
        plt.tight_layout()
        plt.show()
        
        self.logger.info("Training history plotted")